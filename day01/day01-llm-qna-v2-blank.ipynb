{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 - Question and Answers\n",
    "In this workshop, you will learning how to write prompts and feed them into LLMs. You\n",
    "will also be learning how to use different prompt techniques to improve the response\n",
    "from the LLM.\n",
    "\n",
    "## Loading and Explorng the Dataset\n",
    "The workshop will be using [`facebook/ExploreToM`](https://huggingface.co/datasets/facebook/ExploreToM) dataset from [HuggingFace](https://huggingface.co)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the following libraries: datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset_name = \"facebook/ExploreToM\"\n",
    "\n",
    "# TODO: load and explore the dataset\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': (13309, 18)}\n",
      "dict_keys(['train'])\n",
      "{'story_structure': Value(dtype='string', id=None), 'infilled_story': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'expected_answer': Value(dtype='string', id=None), 'qprop=params': Value(dtype='string', id=None), 'qprop=nth_order': Value(dtype='int64', id=None), 'qprop=non_unique_mental_state': Value(dtype='bool', id=None), 'sprop=is_false_belief_story_1st': Value(dtype='bool', id=None), 'sprop=is_false_belief_story_1st_and_2nd': Value(dtype='bool', id=None), 'sprop=story_accuracy_1st_raw': Value(dtype='float64', id=None), 'sprop=story_accuracy_1st_infilled': Value(dtype='float64', id=None), 'sprop=global_idx': Value(dtype='int64', id=None), 'param=story_type': Value(dtype='string', id=None), 'param=num_stories_total': Value(dtype='int64', id=None), 'param=max_sentences': Value(dtype='int64', id=None), 'param=num_people': Value(dtype='int64', id=None), 'param=num_moves': Value(dtype='int64', id=None), 'param=num_rooms': Value(dtype='int64', id=None)}\n",
      "key: story_structure\n",
      "Charlotte entered the grand ballroom. Alexis entered the grand ballroom. Alexis told out loud about the wedding cake design. While this action was happening, Gabriella witnessed this action in secret (and only this action). Gabriella entered the grand ballroom. Charlotte told out loud about the best man's speech. Charlotte left the grand ballroom. Charlotte entered the grand ballroom. Charlotte told out loud about the photo booth props.\n",
      "\n",
      "key: infilled_story\n",
      "The grand ballroom sparkled with elegant chandeliers, its walls adorned in delicate white flowers and the soft glow of candles. Tonight, it was transformed into a magical setting, surrounded by the serene beauty of a garden under the moonlight. As Charlotte stepped into the grand ballroom, her eyes met the warm smile of Alexis, who had arrived just moments before, their simultaneous glances scanning the room to ensure every detail was in place. Alexis's words spilled out like a tantalizing secret, the wedding cake taking shape in the air as she spoke, while Gabriella, hidden from view, absorbed every syllable like a whispered confidence. In the grand ballroom, the whispers of anticipation awaited Gabriella's entrance, her elegant presence marking the defining moment the sanctuary took its breath, waiting in anticipation for the spectacle that was to come. Time paused, listening for the room's silken sigh, a symphony conducted by her serene smile. Suddenly, Charlotte's voice drifted through the ballroom, \"It's now time to hear from the best man, as he shares some unforgettable memories of the groom,\" and with that, all eyes veered toward the solitary figure standing in the spotlight. The sound of Charlotte's voice faded into the distance as she made her way out of the grand ballroom, the whispers of the guests and the gentle hum of the music left to fill the space she had just occupied. Charlotte's eyes widened with a sense of wonder, drinking in the grand ballroom's intricate details as the evening's magic unfolded. Charlotte's booming voice brought attention to another delightful aspect of the evening, the variety of colorful photo booth props waiting to be discovered by the guests.\n",
      "\n",
      "key: question\n",
      "What does Charlotte think about Gabriella's belief on best man's speech? (knows about it / does not know about it)\n",
      "\n",
      "key: expected_answer\n",
      "knows about it\n",
      "\n",
      "key: qprop=params\n",
      "(['Charlotte', 'Gabriella'], \"best man's speech\", '<knowledge>-True')\n",
      "\n",
      "key: qprop=nth_order\n",
      "2\n",
      "\n",
      "key: qprop=non_unique_mental_state\n",
      "False\n",
      "\n",
      "key: sprop=is_false_belief_story_1st\n",
      "True\n",
      "\n",
      "key: sprop=is_false_belief_story_1st_and_2nd\n",
      "True\n",
      "\n",
      "key: sprop=story_accuracy_1st_raw\n",
      "0.555555555555556\n",
      "\n",
      "key: sprop=story_accuracy_1st_infilled\n",
      "0.333333333333333\n",
      "\n",
      "key: sprop=global_idx\n",
      "491\n",
      "\n",
      "key: param=story_type\n",
      "fantom-public+asymmetric\n",
      "\n",
      "key: param=num_stories_total\n",
      "10\n",
      "\n",
      "key: param=max_sentences\n",
      "15\n",
      "\n",
      "key: param=num_people\n",
      "3\n",
      "\n",
      "key: param=num_moves\n",
      "3\n",
      "\n",
      "key: param=num_rooms\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: number of rows in the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# TODO: Keys in the dataset\n",
    "print(dataset.keys())\n",
    "\n",
    "# TODO: Feature names\n",
    "print(dataset['train'].features)\n",
    "\n",
    "# TODO: Display a single row\n",
    "idx = 10000\n",
    "for k, v in dataset['train'][idx].items():\n",
    "   print(f'key: {k}')\n",
    "   print(v)\n",
    "   print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: import pipeline\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `pipeline`\n",
    "[`pipeline`](https://huggingface.co/docs/transformers/en/main_classes/pipelines) is an easy to use API to perform inferencing. It provides a wrapper for task-specific pipelines and abstracts most of the complexity by allowing you to focus on the model and the task. \n",
    "\n",
    "You can use `pipeline` to perform summarisation, image classification, audio generation, etc. You can find an exhaustive list of `pipeline` task [here](https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leslie entered the main tent. Leslie left the main tent. Isabella entered the storage trailer. Isabella moved the stuffed rabbit to the wooden chest, which is also located in the storage trailer. Leslie entered the main tent. Isabella moved the stuffed rabbit to the main tent, leaving the wooden chest in its original location. Isabella told out loud about the festival marketing strategies. Isabella told privately to Colton that Leslie is in the main tent. While this action was happening, Leslie witnessed this action in secret (and only this action).\n",
      "-------------------\n",
      "[{'summary_text': ' Leslie entered the main tent. Isabella moved the stuffed rabbit to the wooden chest, which is also located in the storage trailer . Leslie'}]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Summarise the text with the pipeline's default model\n",
    "idx = 100 \n",
    "summarise = pipeline('summarization')\n",
    "text = dataset['train'][idx]['story_structure']\n",
    "result = summarise(text, min_length=5, max_length=30)\n",
    "print(text)\n",
    "print('-------------------')\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.7939401865005493, 'start': 367, 'end': 384, 'answer': 'leather briefcase'}\n",
      "leather briefcase\n"
     ]
    }
   ],
   "source": [
    "qna = pipeline('question-answering')\n",
    "\n",
    "idx = 5\n",
    "text = dataset['train'][idx]['story_structure']\n",
    "question = dataset['train'][idx]['question']\n",
    "\n",
    "result = qna(context=text, question=question)\n",
    "\n",
    "print(result)\n",
    "\n",
    "print(dataset['train'][idx]['expected_answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Inference - Question and Answer\n",
    "In this section, we will look at what `pipeline` does under the hood to perform its inference. This will give us a better understanding of the major steps involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load tokenizer\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT base cased distilled SQuAD\n",
    "DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. More details [here](https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert/distilbert-base-cased-distilled-squad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1992,  1602, 15430, 24752,  1116,  1602,  1892,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encode text\n",
    "message = 'big black bug bleeds black blood'\n",
    "\n",
    "encoded = tokenizer(message, return_tensors='pt')\n",
    "\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1992,  1602, 15430, 24752,  1116,  1602,  1892,   102,     0,\n",
      "             0,     0],\n",
      "        [  101,  5105,  5105,  5105,  1240,  3499,   117,  4588,  1205,  1103,\n",
      "          5118,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encoding multiple texts\n",
    "texts = [\n",
    "   message,\n",
    "   \"row row row your boat, gently down the stream\"\n",
    "]\n",
    "\n",
    "encoded = tokenizer(texts, return_tensors='pt', padding=True)\n",
    "\n",
    "print(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row row row your boat, gently down the stream\n"
     ]
    }
   ],
   "source": [
    "# TODO: Decode text\n",
    "\n",
    "decoded = tokenizer.decode(encoded['input_ids'][1], skip_special_tokens=True)\n",
    "\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with LLMs\n",
    "Create and instance of the Large Language Model (LLM). We will then create a simple\n",
    "prompt, tokenize the prompt and feed the tokenized prompt to the LLM. The response\n",
    "from the LLM will be decoded to human friendly text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load libraries\n",
    "from transformers import AutoModelForQuestionAnswering \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load question answer model\n",
    "idx = 20 \n",
    "context = dataset['train'][idx]['story_structure']\n",
    "context = dataset['train'][idx]['infilled_story']\n",
    "question = dataset['train'][idx]['question']\n",
    "expected_answer = dataset['train'][idx]['expected_answer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1109,  3336,  1125,  3742, 15495,   117,  9616,   170,  3258,\n",
      "          8656,  1166,  1103, 16118,  1979,  5184,   119,  1130,  1103,  1707,\n",
      "          1395,   117,   170,  5495,  1104,  5444,  5097,   117,  8113,   188,\n",
      "         21200,  1279,   117,  1105,  4301, 18217,  1116,  7648,  1103,  4122,\n",
      "          7072,   117,  4405,  1118,  1103,  7859,  5974,  1104,  1486, 27650,\n",
      "          1105,  4489,  6628,   119,  1109,  1442,   172, 28051,  1501,   117,\n",
      "          1105, 23929,  8495,  1389,  2045,  1107,   117,  1117,  1257, 15049,\n",
      "          1103, 22911,  1707,  1395,   117,  1229,  4899,  1224,   117, 20587,\n",
      "          1145,  2242,  1103,  2000,   117,  1123,  2806, 16708,  1166,  1103,\n",
      "          1395,   112,   188,  9726,   119,  1249, 20587,  3378,  1113, 10883,\n",
      "         16504,  1283,  1103,  8113,  6665,   117,  1123,  2209,  4003,  4359,\n",
      "          1111,   170,  3325,  1248,   117, 23438,  2368, 23929,  8495,  1389,\n",
      "           170,  4142,  2487,  1106,  4432, 22036,  1103,  5444,  1154,  1103,\n",
      "         22823,  2884,   117,  1208,  2355,  1103,  4727,  4768,  5097,  1439,\n",
      "          1157, 26887, 14255, 27084,   119,  1130,  1141, 15085,  4018,   117,\n",
      "         20587,  3687,  5790,  1171,  1154,  1103, 22911,  2343,  1104,  4580,\n",
      "          1105, 22009,   117,  4588,  1231, 19091,  6348,  1158,  1103,  5444,\n",
      "          1121,  1103, 22823,  2884,  1105,   174, 17764,  1157,  8792,  9510,\n",
      "          1154,  1103,  2613,  5439,  2068, 20492,   117, 26533,  1621,  1103,\n",
      "          7648,  4454,  1107,  1103,  1707,  1395,   119,  1188,  4036,  1508,\n",
      "          1123,  1713,  1120,  7166,   117,  5857,  1115,  1142,  3442, 17349,\n",
      "          1618,  1920,  1111,  1147,  9692,  5444,   119,   102,  1130,  1134,\n",
      "         12461,  1108,  1103,  5444,  1120,  1103,  2150,   136,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Encode context and question\n",
    "\n",
    "enc_context = tokenizer(context, return_tensors='pt')\n",
    "enc_question = tokenizer(question, return_tensors='pt')\n",
    "\n",
    "inputs = tokenizer(context, question, return_tensors='pt', padding=True)\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenize the inputs\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(inputs['input_ids'], inputs['attention_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure minimum and maximum token length in the answer\n",
    "def ensure_size(input_ids, answer, min_length = 2, max_length = 5):\n",
    "   ans_start = torch.argmax(answer['start_logits'])\n",
    "   ans_end = torch.argmax(answer['end_logits']) + 1\n",
    "   ans_length = ans_end - ans_start\n",
    "   if ans_length < min_length:\n",
    "      ans_end = min(ans_start + min_length, len(input_ids[0]))\n",
    "   elif ans_length > max_length:\n",
    "      ans_end = ans_start + max_length\n",
    "   ans_start = max(0, ans_start)\n",
    "   ans_end = min(len(input_ids[0]), ans_end)\n",
    "   return (ans_start, ans_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([101])\n"
     ]
    }
   ],
   "source": [
    "#print(result)\n",
    "start_text = torch.argmax(result.start_logits)\n",
    "end_text = torch.argmax(result.end_logits)\n",
    "answer_ids = inputs['input_ids'][0][start_text: end_text + 1 ]\n",
    "print(answer_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " cardboard box\n"
     ]
    }
   ],
   "source": [
    "decoded_ans = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
    "print(decoded_ans, expected_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Return a minimum of 5 tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try this your self\n",
    "\n",
    "context = \"\"\"\n",
    "Dickens wrote A Christmas Carol during a period when the British were exploring and re-evaluating past Christmas traditions, \n",
    "including carols, and newer customs such as cards and Christmas trees. He was influenced by the experiences of his own youth and \n",
    "by the Christmas stories of other authors, including Washington Irving and Douglas Jerrold. Dickens had written three Christmas \n",
    "stories prior to the novella, and was inspired following a visit to the Field Lane Ragged School, one of several establishments for \n",
    "London's street children. The treatment of the poor and the ability of a selfish man to redeem himself by transforming into a more \n",
    "sympathetic character are the key themes of the story. There is discussion among academics as to whether this is a fully secular \n",
    "story or a Christian allegory.\n",
    "\"\"\"\n",
    "\n",
    "question = \"How many stories has Dickens wrote?\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
